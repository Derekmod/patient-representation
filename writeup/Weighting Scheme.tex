\documentclass{article}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{amsthm}
\DeclareMathOperator*{\argmin}{arg\!\min}
\DeclareMathOperator*{\argmax}{arg\!\max}

\geometry{letterpaper, portrait, margin=1in}
\newcommand{\ul}[0]{\underline}
\newcommand{\hs}[1]{\hspace*{#1 cm}}
\newcommand{\ind}[0]{\indent}
\newcommand{\tx}[1]{\text{#1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}

\title{Patient Representation}
\author{Derek Modzelewski}

\begin{document}

\maketitle

\section{Weighting Patients}
Not all patients had the same tissues sampled. Furthermore, not all patients have the same {\it number} of tissues sampled. Thus, we are more confident about the learned representations for some patients (the ones with more samples) than for other patients (the ones with fewer samples). This factors into our prediction of the tissue transform matrices and tissue centers, which then affect the prediction of patient representations.

Instead of solving for the optimal weights directly, we use the weights generated from a related problem which is significantly easier to work with. It is our belief that the weighting scheme will be optimal for our patient representation problem as well.

\theorem Given many estimations $\hat{\theta}_1,\hat{\theta}_2,...,\hat{\theta}_n$ of parameter $\theta_0 \in \mathbb{R}^d$, where
\[ \hat{\theta}_\cdot \equiv (\hat{\theta}_1,\hat{\theta}_2,...,\hat{\theta}_n) = \theta_0\mathbf{1}^T + B + u + r \]
\[ \mathbb{E}[u] = \mathbb{E}[r] = \mathbf{0} \]
where $B$ is the bias, $u$ is the "uncertainty" in the bias and has mean $0$ and covariance matrix $\epsilon$, and $r$ is the "random error" and has mean $0$ and covariance $DVD$ where $D$ is a diagonal matrix where $D_{i,i} = 1/\sqrt{n_i}$ $\forall i \in [n]$, and $u$ and $r$ are independent \\
Then the estimation $\tilde{\theta}$ of $\theta_0$ which minimizes squared loss is given by $w^T\hat{\theta}_\cdot$ where
\[ w = k(2B^TB + \epsilon + DVD)^{-1}\mathbf{1} \]
for suitable $k \in \mathbb{R}$ {\it s.t.} $w^T\mathbf{1} = 1$ \\
\proof any estimation $\delta(\hat{\theta}_\cdot)$ of $\theta_0$ using can be expressed as $\delta(\hat{\theta_\cdot}) = \hat{\theta}_\cdot w + v$ for some $w \in \mathbb{R}^n$ {\it s.t.} $\mathbf{1}^Tw = 1$, some $v \in \mathbb{R}^d$, for given $\hat{\theta}_\cdot$
\begin{align*}
\mathbb{E}\big[||\delta(\hat{\theta}_\cdot) - \theta_0||^2\big] &= \mathbb{E}\big[||\hat{\theta}_\cdot w + v - \theta_0||^2\big]
\\&= \mathbb{E}\big[||\theta_0\mathbf{1}w+Bw + uw + rw + v - \theta_0||^2\big]
\\&= \mathbb{E}\big[||\theta_0+Bw + uw + rw + v - \theta_0||^2\big]
\\&= \mathbb{E}\big[||Bw + uw + rw + v||^2\big]
\\&= \mathbb{E}\big[||(B+u+r)w + v||^2\big]
\\&= \mathbb{E}\big[w^T(B+u+r)^T(B+u+r)w + 2v^T(B+u+r)w + v^Tv\big]
\\&= \mathbb{E}\big[w^T(B+u+r)^T(B+u+r)w\big] + 2v^T\mathbb{E}\big[B+u+r\big]w + v^Tv
\\&= w^T\mathbb{E}\big[(B+u+r)^T(B+u+r)\big]w + 2v^TBw+ v^Tv
\\&= w^T\mathbb{E}\big[B^TB + u^Tu + r^Tr\big]w + 2v^TBw+ v^Tv
\\&= w^T\mathbb{E}(B^TB + \epsilon + DVD)w + 2v^TBw+ v^Tv
\end{align*}
At minimum, the gradient with respect to $w$ and with respect to $v$ will be normal to the feasible spaces of $w$ and $u$, respectively. \\

$v$ has full rank feasible region (no constraints), so only the $0$ vector is normal to its feasible region:
\begin{align*}
\Rightarrow 0 &= \triangledown_v \bigg(\mathbb{E}\big[||\delta(\hat{\theta}_\cdot) - \theta_0||^2\big]\bigg)
\\&= \triangledown_v \Big(w^T(B^TB + \epsilon + DVD)w + 2v^TBw+ v^Tv\Big)
\\&= \triangledown_v \Big(2v^TBw+ v^Tv\Big)
\\&= 2Bw+ 2v
\end{align*}
\[ \Rightarrow v = Bw \]

For $w$, our only constraint is that $\mathbf{1}^Tw = 1$ $\therefore$ forall feasible $w$, all vectors $d$ normal to the feasible region can be expressed $d = k\mathbf{1}$ for some $k \in \mathbb{R}$ \\
\begin{align*}
\Rightarrow k \mathbf{1} &= \triangledown_w \bigg(\mathbb{E}\big[||\delta(\hat{\theta}_\cdot) - \theta_0||^2\big]\bigg)
\\&= \triangledown_w \Big(w^T(B^TB + \epsilon + DVD)w + 2v^TBw+ v^Tv\Big)
\\&= \triangledown_w \Big(w^T(B^TB + \epsilon + DVD)w + 2v^TBw\Big)
\\&= 2(B^TB + \epsilon + DVD)w + (2v^TB)^T
\\&= 2(B^TB + \epsilon + DVD)w + 2B^Tv
\\&= 2(B^TB + \epsilon + DVD)w + 2B^TBw
\\&= 2(2B^TB + \epsilon + DVD)w
\end{align*}
\[ \Rightarrow w = k(2B^TB + \epsilon + DVD)^{-1}\mathbf{1} \]
for appropriate $k$ \\



\section{Weighting Patients}
Not all patients had the same tissues sampled. Furthermore, not all patients have the same {\it number} of tissues sampled. Thus, we are more confident about the learned representations for some patients (the ones with more samples) than for other patients (the ones with fewer samples). This factors into our prediction of the tissue transform matrices and tissue centers, which then affect the prediction of patient representations.

Instead of solving for the optimal weights directly, we use the weights generated from a related problem which is significantly easier to work with. It is our belief that the weighting scheme will be optimal for our patient representation problem as well.

\theorem Given many estimations $\hat{\theta}_1,\hat{\theta}_2,...,\hat{\theta}_n$ of parameter $\theta_0 \in \mathbb{R}^d$, where
\[ \hat{\theta}_\cdot \equiv (\hat{\theta}_1,\hat{\theta}_2,...,\hat{\theta}_n) = \theta_0\mathbf{1}^T + r \]
Then the estimation $\tilde{\theta}$ of $\theta_0$ which minimizes squared loss is given by $\hat{\theta}_\cdot w + v$ where
\[ w = k(\mathbb{E}[r^Tr] - \mathbb{E}[r]^T\mathbb{E}[r])^{-1}\mathbf{1} \]
for suitable $k \in \mathbb{R}$ {\it s.t.} $w^T\mathbf{1} = 1$, and \\
\[ v = -\mathbb{E}[r]w \]
\proof any estimation $\delta(\hat{\theta}_\cdot)$ of $\theta_0$ using can be expressed as $\delta(\hat{\theta_\cdot}) = \hat{\theta}_\cdot w + v$ for some $w \in \mathbb{R}^n$ {\it s.t.} $\mathbf{1}^Tw = 1$, some $v \in \mathbb{R}^d$, for given $\hat{\theta}_\cdot$
\begin{align*}
\mathbb{E}\big[||\delta(\hat{\theta}_\cdot) - \theta_0||^2\big] &= \mathbb{E}\big[||\hat{\theta}_\cdot w + v - \theta_0||^2\big]
\\&= \mathbb{E}\big[||\theta_0\mathbf{1}^Tw+rw + v - \theta_0||^2\big]
\\&= \mathbb{E}\big[||rw + v||^2\big]
\\&= \mathbb{E}\big[w^Tr^Trw + 2v^Trw + v^Tv\big]
\\&= w^T\mathbb{E}[r^Tr]w + 2v^T\mathbb{E}[r]w+ v^Tv
\end{align*}
At minimum, the gradient with respect to $w$ and with respect to $v$ will be normal to the feasible spaces of $w$ and $u$, respectively. \\

$v$ has full rank feasible region (no constraints), so only the $0$ vector is normal to its feasible region:
\begin{align*}
\Rightarrow 0 &= \triangledown_v \bigg(\mathbb{E}\big[||\delta(\hat{\theta}_\cdot) - \theta_0||^2\big]\bigg)
\\&= \triangledown_v \Big(w^T\mathbb{E}[r^Tr]w + 2v^T\mathbb{E}[r]w+ v^Tv\Big)
\\&= \triangledown_v \Big(2v^T\mathbb{E}[r]w+ v^Tv\Big)
\\&= 2\mathbb{E}[r]w+ 2v
\end{align*}
\[ \Rightarrow v = -\mathbb{E}[r]w \]

For $w$, our only constraint is that $\mathbf{1}^Tw = 1$ $\therefore$ forall feasible $w$, all vectors $d$ normal to the feasible region can be expressed $d = k\mathbf{1}$ for some $k \in \mathbb{R}$ \\
\begin{align*}
\Rightarrow k \mathbf{1} &= \triangledown_w \bigg(\mathbb{E}\big[||\delta(\hat{\theta}_\cdot) - \theta_0||^2\big]\bigg)
\\&= \triangledown_w \Big(w^T\mathbb{E}[r^Tr]w + 2v^T\mathbb{E}[r]w+ v^Tv\Big)
\\&= \triangledown_w \Big(w^T\mathbb{E}[r^Tr]w + 2v^T\mathbb{E}[r]w\Big)
\\&= 2\mathbb{E}[r^Tr]w + \big(2v^T\mathbb{E}[r]\big)^T
\\&= 2\mathbb{E}[r^Tr]w + 2\mathbb{E}[r]^Tv
\\&= 2\mathbb{E}[r^Tr]w - 2\mathbb{E}[r]^T\mathbb{E}[r]w
\\&= 2\big(\mathbb{E}[r^Tr] - \mathbb{E}[r]^T\mathbb{E}[r]\big)w
\end{align*}
\[ \Rightarrow w = k(\mathbb{E}[r^Tr] - \mathbb{E}[r]^T\mathbb{E}[r])^{-1}\mathbf{1} \]
for appropriate $k$ \\
\remark The optimal $v$ and $w$ values are independent of the dimension of the parameter space, $d$

\corollary If we restrict our estimation to be in the convex hull of $\{\hat{\theta}_1,...,\hat{\theta}_n\}$, the optimal solution is given by $\hat{\theta_\cdot}w$ where
\[ w = k\mathbb{E}[r^Tr]^{-1}\mathbf{1} \]
for suitable $k$ as before.
\proof Any point in the convex hull of $\{\hat{\theta}_1,...,\hat{\theta}_n\}$ may be expressed as $\hat{\theta_\cdot}w$, which is equivalent to setting $v$ to $\mathbf{0}$. In the previous proof, we arrived at
\begin{align*}
...k\mathbf{1} &= 2\mathbb{E}[r^Tr]w + 2\mathbb{E}[r]^Tv
\\&= 2\mathbb{E}[r^Tr]w &\because v=\mathbf{0}
\end{align*}
\[ \Rightarrow w = k\mathbb{E}[r^Tr]^{-1}\mathbf{1} \]
\remark It may often be convenient to separate $\mathbb{E}[r^Tr] = \mathbb{E}[r]^T\mathbb{E}[r] + Var(r)$, especially for biased estimators.

\corollary In the special case that $r = u + v$ where
\[ \mathbb{E}[u] = \mathbb{E}[v] = \mathbf{0} \]
and $\mathbb{E}[u^Tu] = \phi I$ for some $\phi \in \mathbb{R}$, $\mathbb{E}[u^Tv]=\mathbf{0}$, and $\mathbb{E}[v^Tv]$ is diagonal where $\mathbb{E}[v^Tv]_{i,i} = \gamma/n_i$ $\forall i \in [n]$, for some $\gamma \in \mathbb{R}$, and we are restricted to estimations in the convex hull of $\{\hat{\theta}_1,...,\hat{\theta}_n\}$, the optimal estimator, $\tilde{\theta}$, is
\[ \tilde{\theta} = \frac{\sum_{i=1}^n \frac{n_i}{n_i+\gamma/\phi}\hat{\theta}_i}{\sum_{i=1}^n \frac{n_i}{n_i+\gamma/\phi}} \]
\remark This is equivalent to placing a $n_i/(n_i+\gamma/\phi)$ weight to each estimator $\theta_i$
\proof
\begin{align*}
\mathbb{E}[r^Tr] &= \mathbb{E}[u^Tu] + \mathbb{E}[u^Tv] + \mathbb{E}[v^Tu] + \mathbb{E}[v^Tv]
\\&= \phi I + \mathbb{E}[v^Tv]
\end{align*}
$\therefore \mathbb{E}[r^Tr]$ is diagonal where $\mathbb{E}[r^Tr]_{i,i} = \phi + \gamma/n_i \hs{.3}\forall i \in [n]$ \\
$\Rightarrow \mathbb{E}[r^Tr]^{-1}$ is diagonal where 
\begin{align*}
\mathbb{E}[r^Tr]^{-1}_{i,i} &= \frac{1}{\phi+\gamma/n_i}
\\&\propto \frac{n_i}{n_i+\gamma/\phi}
\end{align*}
Using Corollary @ref?? the weight given to estimator $\theta_i$ is $w_i \propto \frac{n_i}{n_i+\gamma/\phi}$ \\
One interpretation of this is that each of our estimators, $\theta_i$, are biased (and inconsistent) with unknown bias, but each estimator has independent bias with equal variance (in whatever system generates the bias), and variance proportional to the inverse of the number of samples. This is extremely related to our problem of weighting patients, as each patient's samples help estimate our linear representation of patients and tissues, but has unknown bias from other sources (confounders), and it is very reasonable to expect the variance to be proportional to the inverse of the number of samples.












%

\end{document}